<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>1、Flink编程模型</title>
    <url>/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h3 id="一、API抽象级别"><a href="#一、API抽象级别" class="headerlink" title="一、API抽象级别"></a>一、API抽象级别</h3><p>Flink提供不同的API抽象级别进行批/流应用程序开发，如图（来源于官网）</p>
<p><img src="/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/Flink-API抽象级别.png" alt="Flink-API抽象级别"></p>
<ul>
<li><p>最低级别的抽象仅提供处理有状态流，通过Process Function嵌入到DataStream API中。它允许用户自由的处理来自于一个或多个流中的事件，并且使用一致的容错状态。另外，用户可以注册事件事件和处理事件回调，从而使程序可以处理复杂的计算。</p>
</li>
<li><p>在实际应用中，大多数应用程序并不需要上述的低级API，而是通过Flink的核心API，例如DataStream API（有界/无界流）和DataSet API（有界数据集）进行编程。这些友好的API提供了用于数据处理的通用组件，例如用户指定的转换，连接，聚合，窗口，状态等，这些API处理数据的类型对应于相应的编程语言中的类。</p>
<p>低级Process Function API和DataStream API集成到一起，可以对某些操作进行低级别抽象。DataSet API提供有界数据集的处理，如循环/迭代。</p>
</li>
</ul>
<h3 id="二、程序与数据流（Dataflows）"><a href="#二、程序与数据流（Dataflows）" class="headerlink" title="二、程序与数据流（Dataflows）"></a>二、程序与数据流（Dataflows）</h3><ul>
<li><p>Flink程序的基本构建块是流和转换，DataSet在Flink DataSet API内部也是流的一种形式。从概念上讲，流是永无止境的数据记录流，转换是将一个或多个流作为输入的操作，并且将一个或多个流作为结果输出。</p>
<p>当执行程序时，Flink程序会被映射成由流和转换算子组成的数据流，每个数据流都是以一个或多个数据源（sources）开始，以一个或多个接收器（sinks）结束，数据流像有向无环图。</p>
<p><img src="/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/Flink编程模型.png" alt="Flink编程模型" style="zoom:100%;"></p>
<p>通常情况下，程序中的转换（transformations）与数据流中的算子（operators）是一对一的关系，有时，特殊情况下，一个转换（transformations）可能有多个转换算子（operators）。</p>
</li>
</ul>
<h3 id="三、并行数据流（Parallel-Dataflows）"><a href="#三、并行数据流（Parallel-Dataflows）" class="headerlink" title="三、并行数据流（Parallel Dataflows）"></a>三、并行数据流（Parallel Dataflows）</h3><ul>
<li><p>Flink程序内部是并行和分布式的，在执行期间，一个流有一个或多个流分区，并且每个算子（operator）有一个或多个运算操作（operator）子任务（subtasks）。每个运算操作子任务彼此独立运行在不同的线程中，并且可能在不同的机器或容器中运行。</p>
<p>运算操作（operator）子任务（subtasks）的数量就是算子的并行度，流的并行度始终产生于operator，同一个程序的不同算子可能有不同的并行度。</p>
<p><img src="/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/Flink并行度.png" alt="Flink并行度"></p>
</li>
<li><p>流可以在两个算子（operator）间以一对一（转发）模式或者重新分配模式传输数据。</p>
<ul>
<li>一对一流（例如上面图片的Source和map()算子之间）保留元素的分区和顺序，意味着map算子的subtask[1]接收到的元素的顺序与Source算子的Source[1]产生的元素顺序是相同的。</li>
<li>重新分配流（例如在上图中的map()和keyBy()/window()之间，或者keyBy()/window()和Sink()之间）会改变流的分区。每个算子的subtask根据不同的转换将数据发送到不同的下游算子到subtask，例如keyBy()通过散列键（hashing the key）将元素重新分区发送到下游；broadcast()或者rebalance()通过随机重新分区将元素发送到下游。在重新分配流中，元素之间的顺序仅在发送和接收的子subtask之间保持顺序（例如map()的subtask[1]和keyBy()/window()的subtask[2]），所以相同key内是保留元素顺序的，但是不同key到达sink的聚合结果是不确定的。</li>
</ul>
</li>
</ul>
<h3 id="四、窗口（Windows）"><a href="#四、窗口（Windows）" class="headerlink" title="四、窗口（Windows）"></a>四、窗口（Windows）</h3><ul>
<li><p>聚合事件（例如：counts，sums）在流处理上与批处理是不相同的，比如不可能在流上统计计算所有的元素，因为流是无界的，因此在流上做聚合（counts，sums等等）需要用窗口（window）确定元素的范围，就像过去5分钟内的计数或者对最近100个元素求和。</p>
<p>窗口可以是时间驱动的（例如每30秒）或者数据驱动的（例如：每100个元素），窗口的类型分为滚动窗口（不重叠），滑动窗口（有重叠）和会话窗口（由不活动的间隙打断）。</p>
<p><img src="/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/窗口类型.png" alt="窗口类型"></p>
</li>
</ul>
<h3 id="五、时间"><a href="#五、时间" class="headerlink" title="五、时间"></a>五、时间</h3><ul>
<li><p>在流处理程序中引用时间时（例如定义窗口），可以引用不同的时间概念。</p>
<ul>
<li>事件时间（Event Time）是事件创建时的时间，在事件中通常用时间戳表示，Flink通过时间戳分配器（timestamp assigners）访问事件的时间戳。</li>
<li>摄入时间（Ingestion Time）是当事件进入Flink数据流的source算子时的时间。</li>
<li>处理时间（Processing Time）是每个算子的本地时间。</li>
</ul>
<p><img src="/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/时间类型.png" alt="时间类型"></p>
</li>
</ul>
<h3 id="六、有状态操作（Stateful-Operations）"><a href="#六、有状态操作（Stateful-Operations）" class="headerlink" title="六、有状态操作（Stateful Operations）"></a>六、有状态操作（Stateful Operations）</h3><ul>
<li><p>数流中很多操作一次仅能保留一个事件的信息（例如事件解析器），也有一些操作记录多个事件的信息（例如窗口运算符）这些操作被称为有状态的。</p>
<p>有状态操作的状态可以被认为嵌入式k/v存储，状态与有状态操作读取的流是一起被严格分区和分发的。因此，只有在kyBy()函数之后，才可以在键控流上访问k/v状态，并且仅限于当前事件的键关联的值。对齐流和状态的键确保所有状态的更新都是本地操作，从而确保了一致性而没有事物开销，这种对齐方式还允许Flink重新分配并透明的调整流的分区。</p>
<p><img src="/2020/10/23/1%E3%80%81Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/有状态操作.png" alt="有状态操作"></p>
</li>
</ul>
<h3 id="七、容错检查点"><a href="#七、容错检查点" class="headerlink" title="七、容错检查点"></a>七、容错检查点</h3><ul>
<li><p>Flink通过结合流重启和检查点实现容错，检查点与每个输入流点特定点以及对应操作运算符的状态有关，一个数据流通过恢复操作算子的状态和从检查点重启事件，从而保持一致性（完全一致处理语义）。</p>
<p>检查点的时间间隔是在执行过程中权衡容错开销与恢复时间（需要恢复的事件数）的一种做法。</p>
</li>
</ul>
<h3 id="八、流上的批处理"><a href="#八、流上的批处理" class="headerlink" title="八、流上的批处理"></a>八、流上的批处理</h3><ul>
<li>Flink执行批处理程序是流处理程序的一种特殊场景，在这种情况下，流上有界的（元素数量有限）。一个数据集在内部被视为数据流，也适用于以同样的方式处理批处理程序，但少量例外：<ul>
<li>批处理的容错不使用检查点，通过完全恢复流来进行恢复，因为输入是有界的，从而恢复会占更多的开销，但也避免了处理高成本，因为避免使用检查点。</li>
<li>在DataSet API中有状态的操作使用简单内存中/内核外的数据结构，而不用k/v索引。</li>
<li>DataSet API引入了特殊的同步（基于超步）迭代，仅在有限流上才有可能。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB数据同步</title>
    <url>/2020/10/23/MongoDB%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/index/</url>
    <content><![CDATA[<h3 id="一、Sqoop方式（不支持）"><a href="#一、Sqoop方式（不支持）" class="headerlink" title="一、Sqoop方式（不支持）"></a>一、Sqoop方式（不支持）</h3><ul>
<li>1、Sqoop是比较常用的在Hadoop和关系型数据库之间进行数据导入导出的工具，Sqoop1导入导出关系型数据库支持JDBC和direct两种方式，其中direct是通过—direct参数使用相关数据库dump工具进行导入导出，例如mysql的mysqldump和mysqlimport工具，可以更快地向MySQL导入和导出。根据官方文档，Sqoop1支持的关系型数据库以及导入导出的方式如下图所示：</li>
</ul>
<p><img src="/2020/10/23/MongoDB%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/index/sqoop-支持的数据库.png" alt="sqoop-支持的数据库"></p>
<pre><code>可以看出，Sqoop1只支持关系型数据库，其中只对Mysql、Oracle、PostgreSQL支持dump直连方式。
</code></pre><ul>
<li>2、根据官方文档，Sqoop2目前并不适用于生产，Sqoop2支持的连接器：FTP Connector、Generic JDBC Connector、 HDFS Connector、Kafka Connector、Kite Connector、SFTP connector，所以Sqoop2同步MongoDB数据的方案不可行。</li>
</ul>
<p>总结：综上，Sqoop1不支持非关系型数据与hadoop的数据导入导出，而Sqoop2不适用于生产，且不支持非关系型数据库与Hadoop的数据导入导出。所以，Sqoop不适用于MongoDB与Hadoop之间的数据同步。</p>
<a id="more"></a>
<h3 id="二、MongoDump方式"><a href="#二、MongoDump方式" class="headerlink" title="二、MongoDump方式"></a>二、MongoDump方式</h3><p>mongodump是官方提供的一个对数据库进行逻辑导出的备份工具，导出文件为<strong>BSON</strong>二进制格式，无法使用文本编辑工具直接查看。mongodump可以导出mongod或者mongos实例的数据，从集群模式来看，可以备份单实例、副本集、分片集集群。</p>
<ul>
<li><p><strong>1、MongoDump方式：</strong></p>
<p>下载二进制包到相应目录即可：<a href="https://www.mongodb.com/try/download/database-tools" target="_blank" rel="noopener">MongoDB工具包</a></p>
<p>简单示例：</p>
<p>使用mongodump导出bson二进制文件，bsondump再用bsondump转成json文件，再put到HDFS上</p>
<p>全量：</p>
<p>1.1、导出bson文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mongodump -h localhost:27017 -d test -c runoob -o C:\Users\xiaohuan.wang\Desktop</span></pre></td></tr></table></figure>
<p>1.2、转成json文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bsondump C:\Users\xiaohuan.wang\Desktop\test\runoob.bson &gt; C:\Users\xiaohuan.wang\Desktop\dump_test_all.json</span></pre></td></tr></table></figure>
<p>1.3、load到hive临时表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -e "truncate table schema_name.table_name;" hive -e "load data local inpath '$mongodb_data_path/article_inc.json' overwrite into table schema_name.table_name;"</span></pre></td></tr></table></figure>
<p>增量：</p>
<p>使用-q参数对日期或者相关字段进行条件过滤增量导出文件。</p>
</li>
</ul>
<p>  Mongodump使用go写的，大量使用了goroutine，远程备份猜测是通过TCP/IP协议连接的，没有太多研究，具体可参考：</p>
<p>  <a href="https://blog.csdn.net/weixin_34190136/article/details/90625259" target="_blank" rel="noopener">https://blog.csdn.net/weixin_34190136/article/details/90625259</a></p>
<p>  <a href="https://engineering.mongodb.com/post/multiplexing-golang-channels-to-maximize-throughput/" target="_blank" rel="noopener">https://engineering.mongodb.com/post/multiplexing-golang-channels-to-maximize-throughput/</a></p>
<ul>
<li><p><strong>2、mongoexport方式：</strong></p>
<p>2.1、导出json文件</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mongoexport -h localhost:27017 -d test -c runoob --fields title,description,by,url,tags,likes --type json -o C:\Users\xiaohuan.wang\Desktop\test_all.json</span></pre></td></tr></table></figure>
<pre><code>  2.2、load到hive临时表
</code></pre><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -e "truncate table schema_name.table_name;" hive -e "load data local inpath '$mongodb_data_path/article_inc.json' overwrite into table schema_name.table_name;"</span></pre></td></tr></table></figure>
<p>总结：</p>
<ul>
<li><p>1、对于MongoDB实例的逻辑备份工具，mongodump是个不二选择，官方出品，安全稳定性有保障；</p>
</li>
<li><p>2、mongodump较适合数据量较小的备份，相对于数据量较大的情况，备份效率不是太高；</p>
</li>
<li><p>3、mongodump导出时比较消耗服务器性能，而且不支持同时导出多个库。</p>
</li>
<li><p>4、只需要下载MongoDB官方提供的工具包到大数据集群Client节点即可，不需要部署MongoDB Server、Client等服务。</p>
</li>
</ul>
<h3 id="三、DataX"><a href="#三、DataX" class="headerlink" title="三、DataX"></a>三、DataX</h3><p>DataX是一个异构数据源离线同步工具，致力于实现关系型数据库、HDFS、Hive、HBase、MongoDB等各种异构数据源之间的数据同步功能。DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。</p>
<ul>
<li>1、DataX架构图：</li>
</ul>
<p><img src="/2020/10/23/MongoDB%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/index/架构设计.png" alt="架构设计"></p>
<pre><code>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。
</code></pre><ul>
<li><strong>Reader</strong>：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。</li>
<li><strong>Writer</strong>： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。</li>
<li><p><strong>Framework</strong>：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。</p>
</li>
<li><p>2、DataX目前支持的数据源：</p>
</li>
</ul>
<p><img src="/2020/10/23/MongoDB%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/index/DataX支持的数据源.png" alt="DataX支持的数据源"></p>
<ul>
<li><p>3、DataX部署（开源版本不支持集群部署）</p>
<p>环境要求：</p>
<ul>
<li>Linux</li>
<li><a href="http://www.oracle.com/technetwork/cn/java/javase/downloads/index.html" target="_blank" rel="noopener">JDK(1.8以上，推荐1.8)</a></li>
<li><a href="https://www.python.org/downloads/" target="_blank" rel="noopener">Python(推荐Python2.6.X)</a></li>
<li><a href="https://maven.apache.org/download.cgi" target="_blank" rel="noopener">Apache Maven 3.x</a> (Compile DataX)</li>
</ul>
<p>直接下载DataX工具包：<a href="http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz" target="_blank" rel="noopener">DataX下载地址</a>，下载后解压至本地某个目录，进入bin目录，即可运行同步作业：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd  &#123;YOUR_DATAX_HOME&#125;&#x2F;bin</span></pre></td></tr><tr><td class="code"><pre><span class="line">$ python datax.py &#123;YOUR_JOB.json&#125;</span></pre></td></tr></table></figure>
</li>
<li><p>4、MongoDB数据同步简单测试</p>
<p>全量：</p>
<ul>
<li><p>4.1、在MongoDB中插入如下数据：</p>
<p><img src="/2020/10/23/MongoDB%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/index/MongoDB数据.png" alt="MongoDB数据"></p>
</li>
<li><p>4.2、在Hive中建表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orc_table(</span></pre></td></tr><tr><td class="code"><pre><span class="line">title  <span class="keyword">STRING</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">description  <span class="keyword">STRING</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">bys  <span class="keyword">STRING</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">url</span>  <span class="keyword">STRING</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">likes  <span class="keyword">STRING</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC;</span></pre></td></tr></table></figure>
</li>
<li><p>4.3、编辑json文件</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">  &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">      <span class="attr">"job"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="attr">"setting"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">              <span class="attr">"speed"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                  <span class="attr">"byte"</span>:<span class="number">10485760</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">              &#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">              <span class="attr">"errorLimit"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                  <span class="attr">"record"</span>: <span class="number">0</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                  <span class="attr">"percentage"</span>: <span class="number">0.02</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">              &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">          &#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="attr">"content"</span>: [</span></pre></td></tr><tr><td class="code"><pre><span class="line">              &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                  <span class="attr">"reader"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      <span class="attr">"name"</span>: <span class="string">"mongodbreader"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      <span class="attr">"parameter"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						<span class="attr">"address"</span>: [<span class="string">"localhost:27017"</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						<span class="attr">"userName"</span>: <span class="string">""</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						<span class="attr">"userPassword"</span>: <span class="string">""</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						<span class="attr">"dbName"</span>: <span class="string">"test"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						<span class="attr">"collectionName"</span>: <span class="string">"runoob"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"column"</span> : [</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"title"</span>,<span class="attr">"type"</span>: <span class="string">"string"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"description"</span>,<span class="attr">"type"</span>: <span class="string">"string"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						  &#123;<span class="attr">"name"</span>: <span class="string">"bys"</span>,<span class="attr">"type"</span>: <span class="string">"string"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"url"</span>,<span class="attr">"type"</span>: <span class="string">"string"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"likes"</span>,<span class="attr">"type"</span>: <span class="string">"long"</span>&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          ],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"sliceRecordCount"</span>: <span class="number">100000</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                      &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                  &#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">  				<span class="attr">"writer"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      <span class="attr">"name"</span>: <span class="string">"hdfswriter"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      <span class="attr">"parameter"</span>: &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://localhost:9000"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"fileType"</span>: <span class="string">"orc"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"path"</span>: <span class="string">"/user/hive/warehouse/tmp.db/orc_table"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"fileName"</span>: <span class="string">"test"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"column"</span>: [</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"title"</span>,<span class="attr">"type"</span>: <span class="string">"STRING"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"description"</span>,<span class="attr">"type"</span>: <span class="string">"STRING"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">  						  &#123;<span class="attr">"name"</span>: <span class="string">"bys"</span>,<span class="attr">"type"</span>: <span class="string">"STRING"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"url"</span>,<span class="attr">"type"</span>: <span class="string">"STRING"</span>&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                              &#123;<span class="attr">"name"</span>: <span class="string">"likes"</span>,<span class="attr">"type"</span>: <span class="string">"TINYINT"</span>&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          ],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"writeMode"</span>: <span class="string">"append"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                          <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        <span class="attr">"compress"</span>:<span class="string">"NONE"</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                      &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">                &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">              &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">          ]</span></pre></td></tr><tr><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>
</li>
<li><p>4.4、在DataX安装目录下，执行脚本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python2 datax.py ../job/mongo2mysql.json</span></pre></td></tr></table></figure>
</li>
</ul>
<p>运行结果：</p>
<p><img src="/2020/10/23/MongoDB%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/index/DataX运行结果.png" alt="DataX运行结果"></p>
<p>可以看到4条记录从MongoDB里读取出来，写入到Hive失败记录数为0，但是我去查Hive的时候，Hive中并没有数据，我猜测是我本地的Hive是最新版本的3.3，而看源码得知DataX中编译时支持的是1.1.1版本。因为前面读取MongoDB的时候，因为我本地装的是最新版本的4.4，而DataX的MongoDBReader中读取MongoDB的Java包支持到3.2版本，所以一直没法读取出MongoDB的数据。后面升级MongoDB的依赖包以及重装4.2版本的MongoDB才读取出了数据。所以猜测应该是同样的问题，DataX因为很久没有更新的缘故，对最新版本的相关数据源是有问题的，需要下载源码改动相关代码重新编译才能够完好的支持新版本相关的组件。</p>
</li>
</ul>
<p>  增量：</p>
<p>  通过reader里添加—query参数设置过滤条件实现增量同步</p>
<p>总结：</p>
<p>1、开源版本DataX不支持集群部署模式，且社区不活跃。</p>
<p>2、只需在Client节点解压安装包，写Reader和Writer json格式的任务文件即可运行。</p>
<p>3、使用DataX，就要维护两套数据同步任务，工作量增加。</p>
<h3 id="四、其他方式"><a href="#四、其他方式" class="headerlink" title="四、其他方式"></a>四、其他方式</h3><ul>
<li><p>1、<a href="https://www.mongodb.com/products/spark-connector" target="_blank" rel="noopener">MongoDB Connector for Spark</a></p>
<p>MongoDB Connector for Spark提供MongoDB和Spark之间的集成，可以通过Spark连接MongoDB读取相关数据，再讲数据写到Hive。</p>
<p>参考：<a href="https://docs.mongodb.com/spark-connector/master/" target="_blank" rel="noopener">https://docs.mongodb.com/spark-connector/master/</a></p>
<p>示例：</p>
<p>添加依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mongodb.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mongo-spark-connector_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></pre></td></tr></table></figure>
<p>读取数据：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mongodb.spark_examples;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.bson.Document;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.spark.MongoSpark;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.spark.rdd.api.java.JavaMongoRDD;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFromMongoDB</span> </span>&#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    SparkSession spark = SparkSession.builder()</span></pre></td></tr><tr><td class="code"><pre><span class="line">      .master(<span class="string">"local"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">      .appName(<span class="string">"MongoSparkConnectorIntro"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">      .config(<span class="string">"spark.mongodb.input.uri"</span>, <span class="string">"mongodb://127.0.0.1/test.myCollection"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">      .config(<span class="string">"spark.mongodb.output.uri"</span>, <span class="string">"mongodb://127.0.0.1/test.myCollection"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">      .getOrCreate();</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">// Create a JavaSparkContext using the SparkSession's SparkContext object</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    JavaSparkContext jsc = <span class="keyword">new</span> JavaSparkContext(spark.sparkContext());</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">/*Start Example: Read data from MongoDB************************/</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    JavaMongoRDD&lt;Document&gt; rdd = MongoSpark.load(jsc);</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">/*End Example**************************************************/</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="comment">// Analyze data from MongoDB</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    System.out.println(rdd.count());</span></pre></td></tr><tr><td class="code"><pre><span class="line">    System.out.println(rdd.first().toJson());</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    jsc.close();</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>
</li>
<li><p>2、Flink DataSet 读取MongoDB</p>
<p>参考：<a href="https://github.com/okkam-it/flink-mongodb-test" target="_blank" rel="noopener">https://github.com/okkam-it/flink-mongodb-test</a></p>
<p>添加依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mongodb<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mongo-hadoop-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mongodb.hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr></table></figure>
<p>读取数据：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.okkam.flink.mongodb.test;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.JobConf;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.bson.BSONObject;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.BasicDBObject;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.hadoop.io.BSONWritable;</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.hadoop.mapred.MongoInputFormat;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongodbExample</span> </span>&#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// set up the execution environment</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// create a MongodbInputFormat, using a Hadoop input format wrapper</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		HadoopInputFormat&lt;BSONWritable, BSONWritable&gt; hdIf = </span></pre></td></tr><tr><td class="code"><pre><span class="line">				<span class="keyword">new</span> HadoopInputFormat&lt;BSONWritable, BSONWritable&gt;(<span class="keyword">new</span> MongoInputFormat(),</span></pre></td></tr><tr><td class="code"><pre><span class="line">						BSONWritable<span class="class">.<span class="keyword">class</span>, <span class="title">BSONWritable</span>.<span class="title">class</span>,	<span class="title">new</span> <span class="title">JobConf</span>())</span>;</span></pre></td></tr><tr><td class="code"><pre><span class="line">	</span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// specify connection parameters</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		hdIf.getJobConf().set(<span class="string">"mongo.input.uri"</span>, </span></pre></td></tr><tr><td class="code"><pre><span class="line">				<span class="string">"mongodb://localhost:27017/dbname.collectioname"</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">		DataSet&lt;Tuple2&lt;BSONWritable, BSONWritable&gt;&gt; input = env.createInput(hdIf);</span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// a little example how to use the data in a mapper.</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		DataSet&lt;Tuple2&lt; Text, BSONWritable&gt;&gt; fin = input.map(</span></pre></td></tr><tr><td class="code"><pre><span class="line">				<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;BSONWritable, BSONWritable&gt;, </span></pre></td></tr><tr><td class="code"><pre><span class="line">									Tuple2&lt;Text,BSONWritable&gt; &gt;() &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">					<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">					<span class="meta">@Override</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">					<span class="function"><span class="keyword">public</span> Tuple2&lt;Text,BSONWritable&gt; <span class="title">map</span><span class="params">(</span></span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="params">							Tuple2&lt;BSONWritable, BSONWritable&gt; record)</span> <span class="keyword">throws</span> Exception </span>&#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">						BSONWritable value = record.getField(<span class="number">1</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">						BSONObject doc = value.getDoc();</span></pre></td></tr><tr><td class="code"><pre><span class="line">						BasicDBObject jsonld = (BasicDBObject) doc.get(<span class="string">"jsonld"</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">						</span></pre></td></tr><tr><td class="code"><pre><span class="line">						String id = jsonld.getString(<span class="string">"@id"</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">						DBObject builder = BasicDBObjectBuilder.start()</span></pre></td></tr><tr><td class="code"><pre><span class="line">				                .add(<span class="string">"id"</span>, id)</span></pre></td></tr><tr><td class="code"><pre><span class="line">				                .add(<span class="string">"type"</span>, jsonld.getString(<span class="string">"@type"</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">				                .get();</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">				        BSONWritable w = <span class="keyword">new</span> BSONWritable(builder);</span></pre></td></tr><tr><td class="code"><pre><span class="line">		                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Text,BSONWritable&gt;(<span class="keyword">new</span> Text(id), w);</span></pre></td></tr><tr><td class="code"><pre><span class="line">					&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">				&#125;);</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// emit result (this works only locally)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">//		fin.print();</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		</span></pre></td></tr><tr><td class="code"><pre><span class="line">		MongoConfigUtil.setOutputURI( hdIf.getJobConf(), </span></pre></td></tr><tr><td class="code"><pre><span class="line">				<span class="string">"mongodb://localhost:27017/test.testData"</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// emit result (this works only locally)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		fin.output(<span class="keyword">new</span> HadoopOutputFormat&lt;Text,BSONWritable&gt;(</span></pre></td></tr><tr><td class="code"><pre><span class="line">				<span class="keyword">new</span> MongoOutputFormat&lt;Text,BSONWritable&gt;(), hdIf.getJobConf()));</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">		<span class="comment">// execute program</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">		env.execute(<span class="string">"Mongodb Example"</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">	&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h3><p>通过调研，确定Sqoop不适用于MongoDB的数据集成，其中可行的方案有MongoDB Dump方式、DataX、以及通过现有的计算框架连接读取，目前以上的方案都没有做具体的测试，生产中使用需要根据具体的情况做相应的测试。</p>
]]></content>
      <categories>
        <category>数据采集</category>
      </categories>
      <tags>
        <tag>数据采集</tag>
        <tag>mongodb</tag>
        <tag>datax</tag>
      </tags>
  </entry>
</search>
